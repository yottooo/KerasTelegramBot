{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import Keras \n",
    "from keras import backend as K\n",
    "from keras.utils import generic_utils\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Input,Dropout ,Dense, Conv1D, MaxPooling1D, Flatten\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Embedding\n",
    "\n",
    "import tensorflow as tf\n",
    "#import pands and nympy for data preprocesing \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#Pickle for loading the big model\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splittest set\n",
    "MAX_NB_WORDS = 5000 # consider to 20,000 most occuring words in dataset\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "# load the model from disk\n",
    "filename = 'x1.sav'\n",
    "x1 = pickle.load(open(filename, 'rb'))\n",
    "global graph\n",
    "graph = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import twitter API\n",
    "from twython import Twython\n",
    "from twython import TwythonStreamer\n",
    "from twitter import *\n",
    "APP_KEY=\"\"\n",
    "APP_SECRET=\"\"\n",
    "OAUTH_TOKEN=\"\"\n",
    "OAUTH_TOKEN_SECRET=\"\"\n",
    "#Telegram API\n",
    "import telegram\n",
    "import logging\n",
    "from telegram.ext import Updater, CommandHandler, MessageHandler, Filters\n",
    "#pymongo library for accesing the database\n",
    "import pymongo\n",
    "from pymongo import MongoClient, TEXT, DESCENDING\n",
    "from datetime import datetime, timedelta\n",
    "from simple_settings import settings\n",
    "#Bot information\n",
    "logging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "                     level=logging.INFO)\n",
    "#Input token from Telegram\n",
    "updater = Updater(token='')\n",
    "dispatcher = updater.dispatcher\n",
    "#set up conection to database\n",
    "client = MongoClient()\n",
    "client = MongoClient('localhost', 27017)\n",
    "#connect to database\n",
    "db = client.yo\n",
    "#select collection\n",
    "cfeedback=db.feedback\n",
    "#Global variable for storing feedback\n",
    "fb = \"\"\n",
    "class MyStreamer(TwythonStreamer):\n",
    "    def on_success(self, data):\n",
    "        if 'text' in data:\n",
    "            global job\n",
    "            global model\n",
    "            sequences= tokenizer.texts_to_sequences([data['text']])\n",
    "            dat = pad_sequences(sequences, maxlen=1000)\n",
    "            with graph.as_default():\n",
    "                prediction1 = x1.predict(dat, batch_size=64)[0,1]\n",
    "                prediction2 = model.predict(dat, batch_size=64)[0,1]\n",
    "                prediction= (prediction1+prediction2)/2\n",
    "        if prediction >0.4:\n",
    "            job=data\n",
    "            self.disconnect()\n",
    "\n",
    "    def on_error(self, status_code, data):\n",
    "        print(status_code)\n",
    "\n",
    "stream = MyStreamer(APP_KEY, APP_SECRET,\n",
    "                    OAUTH_TOKEN, OAUTH_TOKEN_SECRET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define comands\n",
    "#Show add on start\n",
    "def start(bot, update):\n",
    "    global updater\n",
    "    global fb\n",
    "    global job\n",
    "\n",
    "    stream.statuses.filter(track='Hiring, CareerArc')\n",
    "    bot.send_message(chat_id=update.message.chat_id, text=\"Please rate this 1 or 0\")\n",
    "    bot.send_message(chat_id=update.message.chat_id, text=job[\"text\"])    \n",
    "    fb=job[\"text\"]\n",
    "\n",
    "#Collect feedback get text from previous post and insert into database with user feedback\n",
    "def collect(msg,uid):\n",
    "    global fb\n",
    "    global model\n",
    "#    global job\n",
    "    feedback={\"User_id\":uid,\n",
    "         \"text\":fb,\n",
    "         \"feedback\":msg}    \n",
    "    cfeedback.insert_one(feedback)\n",
    "    with graph.as_default():\n",
    "        retrain()\n",
    "#For  unknown message and collecting feedback\n",
    "def echo(bot, update):\n",
    "    if update.message.text=='1':        \n",
    "        collect(1,update.message.chat_id)\n",
    "        bot.send_message(chat_id=update.message.chat_id, text=\"----------------------------------------->\")\n",
    "        start(bot, update)\n",
    "    elif update.message.text=='0':\n",
    "        collect(0,update.message.chat_id)\n",
    "        bot.send_message(chat_id=update.message.chat_id, text=\"----------------------------------------->\")\n",
    "        start(bot, update)\n",
    "    else:\n",
    "        bot.send_message(chat_id=update.message.chat_id, text=\"Answer with 1 for true and 0 for false\")\n",
    "        bot.send_message(chat_id=update.message.chat_id, text=\"----------------------------------------->\")\n",
    "        start(bot, update)\n",
    "\n",
    "#for unknown comand message.chat_id=id of the user get it dynamicaly\n",
    "def unknown(bot, update):\n",
    "    bot.send_message(chat_id=update.message.chat_id, text=\"Sorry, I didn't understand that command.\")\n",
    "\n",
    "#Initiate the comands\n",
    "start_handler = CommandHandler('start', start)\n",
    "dispatcher.add_handler(start_handler)\n",
    "#unknown message\n",
    "echo_handler = MessageHandler(Filters.text, echo)\n",
    "dispatcher.add_handler(echo_handler)\n",
    "#unknown comand\n",
    "unknown_handler = MessageHandler(Filters.command, unknown)\n",
    "dispatcher.add_handler(unknown_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start the bot\n",
    "updater.start_polling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stop the bot\n",
    "updater.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for second model\n",
    "def retrain():\n",
    "    #We get our collection from the database and save it in a pandas dataframe\n",
    "    df = pd.DataFrame(list(cfeedback.find()))\n",
    "    labels = df['feedback']\n",
    "    texts = df['text']\n",
    "\n",
    "    MAX_NB_WORDS = 5000 # consider to 20,000 most occuring words in dataset\n",
    "    MAX_SEQUENCE_LENGTH = 1000 # truncate sequences to a maximum length of 1000 words\n",
    "    VALIDATION_SPLIT = 0.2\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "    labels = to_categorical(labels)\n",
    "\n",
    "    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    # split the data into a training set and a validation set\n",
    "    indices = np.arange(data.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    data = data[indices]\n",
    "\n",
    "    nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "    x_train = data[:-nb_validation_samples]\n",
    "    y_train = labels[:-nb_validation_samples]\n",
    "    x_val = data[-nb_validation_samples:]\n",
    "    y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "    embeddings_index = {}\n",
    "\n",
    "    embedding_layer = Embedding(len(word_index) + 1, 100, input_length=MAX_SEQUENCE_LENGTH)\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    # 3 hidden layers with 128 neurons each\n",
    "    x = Conv1D(64, 5, activation='relu')(embedded_sequences)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "    x = Conv1D(64, 5, activation='relu')(x)\n",
    "    #x = Dropout(0.3)(x)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "    x = Conv1D(64, 5, activation='relu')(x)\n",
    "    #x = Dropout(0.2)(x)\n",
    "    x = MaxPooling1D(35)(x)  # global max pooling\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    preds = Dense(2, activation='softmax')(x) # 2 ... binary\n",
    "    global model\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics= ['acc'])\n",
    "    # 10 epochs\n",
    "    model_hist = model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "              epochs=10, batch_size=32)\n",
    "retrain()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
